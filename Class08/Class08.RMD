---
title: "Bioinformatics Class 8"
author: "Jason Patrick Bennett"
date: "April 26, 2018"
output:
  html_document:
    keep_md: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## K-Means Example

Making up data to try K-means:

```{r}
tmp <- c(rnorm(30, -3), rnorm(30, 3) )
x <- cbind(x=tmp, y=rev(tmp))

plot(x)
```


Our tasks are:

1. Use kmeans() function setting k to 2 and nstart to 20.
2. Inspect/print the results

Questions:
  a. How many points are in each cluster?                       30 in each
  b. What "component" of your result object details:
    - Cluster size?                                             km$size
    - Cluster assignment/membership?                            km$cluster
    - Cluster center?                                           km$centers

```{r}
km <- kmeans(x, centers=2, nstart=20)

km
```


Checking the cluster component:

```{r}
km$cluster
```


Checking the points in each cluster (size):

```{r}
km$size
```


Checking cluster centers:

```{r}
km$centers
```


Plot x colored by the kmeans cluster assignment and add cluster centers as blue points

```{r}
plot(x, col=km$cluster)

points(km$centers, col="blue", pch=15)
```


SS? Use tot.withinss

```{r}
km$tot.withinss
```




Repeat for k is 3; which one has the better total SS?

```{r}
km3 <- kmeans(x, centers=3, nstart=20)
km3
```


The "tot.withinss" is the value we are looking for here to evaluate best SS

```{r}
km3$tot.withinss
```











## Hierarchical Clustering

First we need to calculate the point (dis)similarity as the Euclidian distance between observations

```{r}
dist_matrix <- dist(x)
dist_matrix
```


Lets look at the class of this distance matrix

```{r}
class(dist_matrix)

# Can't view it normally, so we need to force it into a matrix: as.matrix()
View(as.matrix(dist_matrix))
dim(as.matrix(dist_matrix))
```


Then use the hclust() function to return a hierarchical clustering model

```{r}
hc <- hclust(d=dist_matrix)

hc
```


Lets try to plot hc:

```{r}
plot(hc)
```


To determine the number of clusters, look at the groupings we have and set an ab-line where we want to define the number of clusters.

ex. At Height 6 above, we would have **2** clusters. At Height 4, we would have **4** clusters.

```{r}
plot(hc)
abline(h=4, col="red")

# This cuts the tree at the height we specify and returns a vector
cutree(hc, h=4)

# Print out a graph with the four clusters clearly shown
plot(x, col=cutree(hc, h=4), pch=16 )
```


Different hclust methods:

```{r}
hc.complete <- hclust(d=dist_matrix, method="complete")

hc.average <- hclust(d=dist_matrix, method="average")

hc.single <- hclust(d=dist_matrix, method="single")

hc.complete
hc.average
hc.single

plot(hc.complete)
plot(hc.average)
plot(hc.single)
```







## Hands-On Application

Generate sample data

```{r}
x <- rbind(
 matrix(rnorm(100, mean=0, sd = 0.3), ncol = 2), # c1
 matrix(rnorm(100, mean = 1, sd = 0.3), ncol = 2), # c2
 matrix(c(rnorm(50, mean = 1, sd = 0.3), # c3
 rnorm(50, mean = 0, sd = 0.3)), ncol = 2))
colnames(x) <- c("x", "y")
```


Plot the data wiithout clustering

```{r}
plot(x)
```


Generate colors for known clusters

```{r}
col <- as.factor( rep(c("c1", "c2", "c3"), each=50) )
```


Plot using our generated color vector

```{r}
plot(x, col=col)
```


Now find distance, hclust, plot, and cutree:

```{r}
temp_matrix <- dist(x)
hc2 <- hclust(d=temp_matrix)
plot(hc2)
clusters2 <- cutree(hc2, k=2)
plot(x, col=clusters2, pch=16)

```


Now plot and check our results:

```{r}
clusters3 <- cutree(hc2, k=3)
plot(x, col=clusters3, pch=16)
```









## PCA Applications


Generate sample data:

```{r}
mydata <- matrix(nrow=100, ncol=10)

rownames(mydata) <- paste("gene", 1:100, sep="")

colnames(mydata) <- c( paste("wt", 1:5, sep=""),
                       paste("ko", 6:10, sep="") )
```


Fill in some fake read counts:

```{r}
for (i in 1:nrow(mydata) )
{
  wt.values <- rpois(5, lambda = sample(x=10:1000, size = 1) )
  ko.values <- rpois(5, lambda = sample(x=10:1000, size = 1) )
  
  mydata[i,] <- c(wt.values, ko.values)
}

head(mydata)
```


Now we can do the PCA! **Note:** prcomp() expects samples to be rows instead of columns, so we have to take the transpose of our data

```{r}
pca <- prcomp(t(mydata), scale=TRUE)
```


Check what is returned by prcomp():

```{r}
attributes(pca)
```


The "x" portion of pca (accessed by pca$x) is what we use to plot. This generates our PC1 v. PC2 plot:

```{r}
plot(pca$x[,1], pca$x[,2])
```


Use the square of the std. dev to calculate the variation: (psa$sdev)^2

```{r}
pca.var <- pca$sdev^2

# Roound the percentage
pca.var.per <- round(pca.var/sum(pca.var) * 100, 1)
```


Make a Scree Plot of the data:

```{r}
barplot(pca.var.per, main="Scree Plot", xlab="Principal Component", ylab="Percent Variation")
```


Clearly, the Scree Plot shows that the difference between our data is significant! Now lets go back to the first graph and add color to help visualize the difference:

```{r}
colvec <- colnames(mydata)
colvec[grep("wt", colvec)] <- "red"
colvec[grep("ko", colvec)] <- "blue"

plot(pca$x[,1], pca$x[,2], col=colvec, pch=16, xlab=paste0("PC1 (", pca.var.per[1], "%)"),
     ylab=paste0("PC2 (", pca.var.per[2], "%)"))

# Add functionality to click to identify points:
identify(pca$x[,1], pca$x[,2], labels=colnames(mydata))
```


